{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hihihi-svg/first-repl/blob/main/RLHF_SPEECH_SKETCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J4CqN2yXnm2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee6a237-1c57-4a77-b8ff-57f1fd807c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhH5ge9IXnZ-"
      },
      "outputs": [],
      "source": [
        "noise_dim = 100\n",
        "cond_dim = 5   # eye_size, openness, shape, tilt, eyebrow\n",
        "image_size = (32, 64)  # H x W"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # [-1, 1]\n",
        "])\n"
      ],
      "metadata": {
        "id": "M-_7q4Kcousi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/celeba-dataset/img_align_celeba/img_align_celeba\"  # folder containing face images\n",
        "\n",
        "image_paths = [\n",
        "    os.path.join(DATASET_PATH, img)\n",
        "    for img in os.listdir(DATASET_PATH)\n",
        "    if img.lower().endswith((\".jpg\", \".png\"))\n",
        "]\n",
        "\n",
        "print(\"Total images:\", len(image_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlI8UypkoyAw",
        "outputId": "a055b8de-196d-490b-d8d5-01246090fc06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 202599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "MAX_TRAIN = 1000\n",
        "MAX_TEST = 150\n",
        "\n",
        "random.shuffle(image_paths)\n",
        "\n",
        "train_paths = image_paths[:MAX_TRAIN]\n",
        "test_paths  = image_paths[MAX_TRAIN:MAX_TRAIN + MAX_TEST]\n",
        "\n",
        "print(\"Train:\", len(train_paths))\n",
        "print(\"Test:\", len(test_paths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHRf0Aq6pEJv",
        "outputId": "5adb247d-9ea8-499e-fe3b-547f57fafe6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1000\n",
            "Test: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "enP9PrLarXa0",
        "outputId": "eb95007f-0ca6-4beb-ed52-83ac9638b66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mediapipe'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-957869527.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eye_cascade = cv2.CascadeClassifier(\n",
        "    cv2.data.haarcascades + 'haarcascade_eye.xml'\n",
        ")\n",
        "\n",
        "def crop_eyes_opencv(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "    if len(eyes) == 0:\n",
        "        return None\n",
        "\n",
        "    # Take the largest eye region\n",
        "    x, y, w, h = max(eyes, key=lambda e: e[2]*e[3])\n",
        "    eye = gray[y:y+h, x:x+w]\n",
        "\n",
        "    return Image.fromarray(eye)\n"
      ],
      "metadata": {
        "id": "vxqt7M07rO4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_eye_features(eye_img):\n",
        "    arr = np.array(eye_img)\n",
        "    h, w = arr.shape\n",
        "\n",
        "    # Eye size\n",
        "    area = h * w\n",
        "    eye_size = 2 if area > 1800 else 0 if area < 900 else 1\n",
        "\n",
        "    # Openness\n",
        "    openness = 0 if h/w < 0.35 else 2 if h/w > 0.55 else 1\n",
        "\n",
        "    # Shape\n",
        "    shape = 0 if w/h < 1.6 else 1  # round / almond\n",
        "\n",
        "    # Tilt (ignored for now â†’ neutral)\n",
        "    tilt = 1\n",
        "\n",
        "    # Eyebrow thickness (dark pixels)\n",
        "    eyebrow = 2 if np.mean(arr[:int(h*0.2)]) < 100 else 1\n",
        "\n",
        "    vec = torch.tensor([eye_size, openness, shape, tilt, eyebrow], dtype=torch.float32)\n",
        "    return vec / 2.0\n"
      ],
      "metadata": {
        "id": "hoiCl8l6irbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop_eyes=crop_eyes_opencv\n",
        "class EyeDataset(Dataset):\n",
        "    def __init__(self, paths, transform):\n",
        "        self.paths = paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ðŸ‘‡ CROPPING HAPPENS HERE\n",
        "        eye_img = crop_eyes_opencv(self.paths[idx])\n",
        "\n",
        "        if eye_img is None:\n",
        "            return self.__getitem__((idx + 1) % len(self.paths))\n",
        "\n",
        "        # ðŸ‘‡ FEATURE EXTRACTION\n",
        "        cond = extract_eye_features(eye_img)\n",
        "\n",
        "        # ðŸ‘‡ TRANSFORM TO TENSOR\n",
        "        img = self.transform(eye_img)\n",
        "\n",
        "        return img, cond"
      ],
      "metadata": {
        "id": "f-9b_uJCiubO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "train_dataset = EyeDataset(train_paths, transform)\n",
        "test_dataset  = EyeDataset(test_paths, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "YWmWAGKmixQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EyeGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(noise_dim + cond_dim, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 4, 1, 0),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 1, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, cond):\n",
        "        x = torch.cat([z, cond], dim=1)\n",
        "        x = self.fc(x)\n",
        "        return self.net(x.view(-1, 512, 1, 1))"
      ],
      "metadata": {
        "id": "s1DAS25xtYpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EyeDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.AdaptiveAvgPool2d((4,4))\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64*4*4 + cond_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, cond):\n",
        "        x = self.conv(img).view(img.size(0), -1)\n",
        "        return self.fc(torch.cat([x, cond], dim=1))"
      ],
      "metadata": {
        "id": "nnqFqp7wtbd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = EyeGenerator().to(device)\n",
        "D = EyeDiscriminator().to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for real, cond in train_loader:\n",
        "        real, cond = real.to(device), cond.to(device)\n",
        "        bs = real.size(0)\n",
        "\n",
        "        valid = torch.ones(bs,1).to(device)\n",
        "        fake  = torch.zeros(bs,1).to(device)\n",
        "\n",
        "        # Generator\n",
        "        z = torch.randn(bs, noise_dim).to(device)\n",
        "        fake_img = G(z, cond)\n",
        "        g_loss = criterion(D(fake_img, cond), valid)\n",
        "        opt_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        # Discriminator\n",
        "        real_loss = criterion(D(real, cond), valid)\n",
        "        fake_loss = criterion(D(fake_img.detach(), cond), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        opt_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch} | G:{g_loss.item():.3f} D:{d_loss.item():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Cs54gjteHJ",
        "outputId": "3cb2b9c9-8ecd-4d01-91c9-68ccfe0fef05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | G:1.733 D:0.311\n",
            "Epoch 10 | G:5.692 D:0.004\n",
            "Epoch 20 | G:6.701 D:0.001\n",
            "Epoch 30 | G:7.509 D:0.001\n",
            "Epoch 40 | G:9.021 D:0.000\n",
            "Epoch 50 | G:9.791 D:0.000\n",
            "Epoch 60 | G:10.093 D:0.000\n",
            "Epoch 70 | G:9.561 D:0.000\n",
            "Epoch 80 | G:11.088 D:0.000\n",
            "Epoch 90 | G:11.312 D:0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_eye_features(text):\n",
        "    text = text.lower()\n",
        "    eye_size = 2 if \"big\" in text else 0 if \"small\" in text else 1\n",
        "    openness = 0 if \"narrow\" in text else 2 if \"wide\" in text else 1\n",
        "    shape = 0 if \"round\" in text else 1\n",
        "    tilt = 1\n",
        "    eyebrow = 2 if \"thick\" in text else 1\n",
        "    return torch.tensor([eye_size, openness, shape, tilt, eyebrow]) / 2.0\n"
      ],
      "metadata": {
        "id": "TWJxWg5rto8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G.eval()\n",
        "cond = text_to_eye_features(\"big round eyes with thick eyebrows\").unsqueeze(0).to(device)\n",
        "z = torch.randn(1, noise_dim).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    img = G(z, cond)\n",
        "\n",
        "plt.imshow(((img.squeeze().cpu()+1)/2), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "-1f7hMo6tsHk",
        "outputId": "841aba5d-ccfe-4be8-aa3f-db7512ca1d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle/\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcu-u5-Vi6nz",
        "outputId": "a49cd249-9450-4a6c-cad7-9dd81eedf7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\n",
            "License(s): other\n",
            "Downloading celeba-dataset.zip to /content\n",
            "100% 1.33G/1.33G [00:08<00:00, 221MB/s]\n",
            "100% 1.33G/1.33G [00:08<00:00, 166MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/celeba-dataset.zip', 'r')\n",
        "zip_ref.extractall('/content/celeba-dataset')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "Nz7gl_iIi6k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yd3zyXxCi6iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zJ4O5Lxi6fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUNhUoYRi6a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2OFb-O8i6X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QXLKOPYi6U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "md3IPmrAi6Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCvkcLY7i6Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_LuRk76Wi6Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_qqDd0yBb0h"
      },
      "outputs": [],
      "source": [
        "TARGET_W = 256\n",
        "PART_H = 85   # roughly 1/3 of 256\n",
        "\n",
        "eyes = Image.open(\"eyes.png\").resize((TARGET_W, PART_H))\n",
        "nose = Image.open(\"nose.png\").resize((TARGET_W, PART_H))\n",
        "mouth = Image.open(\"mouth.png\").resize((TARGET_W, PART_H))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_INTGKrBxok"
      },
      "outputs": [],
      "source": [
        "collage = Image.new(\"RGB\", (TARGET_W, PART_H * 3), \"white\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esTw2ZsRBttP"
      },
      "outputs": [],
      "source": [
        "collage.paste(eyes, (0, 0))                 # top\n",
        "collage.paste(nose, (0, PART_H))            # middle\n",
        "collage.paste(mouth, (0, PART_H * 2))       # bottom\n",
        "\n",
        "collage.save(\"collaged_face.png\")\n",
        "collage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Koio0-F36SeR"
      },
      "outputs": [],
      "source": [
        "#PIX2PIX GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATMfpjbp7Fh3",
        "outputId": "e2e4fd4d-fdca-4a61-e5e1-cd171ac95660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision pillow matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E4pHHFbEh2s",
        "outputId": "36e7a74b-40a6-48cf-c8de-cb8d8a85b80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIfte4gvE9k9"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "iQX-JJHNFA9k",
        "outputId": "cdef0a14-db70-44a8-a70c-721320058f9f"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'collaged_face.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1071349215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcollage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collaged_face.png\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# INPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1071349215.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcollage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collaged_face.png\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# INPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'collaged_face.png'"
          ]
        }
      ],
      "source": [
        "def load_image(path):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    return transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "collage = load_image(\"collaged_face.png\")  # INPUT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "gZVyTARgFF5W",
        "outputId": "f4ad3a91-c799-4027-eb04-b9fb67de74c2"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'collaged_face.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1071349215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcollage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collaged_face.png\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# INPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1071349215.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcollage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collaged_face.png\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# INPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'collaged_face.png'"
          ]
        }
      ],
      "source": [
        "def load_image(path):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    return transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "collage = load_image(\"collaged_face.png\")  # INPUT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbEkJwJfFM7m"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLtLczo7FPZr"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(6, 64, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 1, 4, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x = collage, y = real or fake face\n",
        "        return self.net(torch.cat([x, y], dim=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyXEvJqGFSdB"
      },
      "outputs": [],
      "source": [
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzBIF6o2FZQ1"
      },
      "outputs": [],
      "source": [
        "criterion_GAN = nn.BCELoss()\n",
        "criterion_L1 = nn.L1Loss()\n",
        "\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "eemAKEp7FeXR",
        "outputId": "96364ab7-455a-470c-a6a7-8120a771380b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'collage' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3335896747.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# replace with real refined face later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'collage' is not defined"
          ]
        }
      ],
      "source": [
        "target_face = collage.clone()  # replace with real refined face later\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "iJ37CWxwFiJY",
        "outputId": "9464c78f-c054-4785-af7f-e5b590c115c4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'collage' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4231711149.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfake_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpred_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_face\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'collage' is not defined"
          ]
        }
      ],
      "source": [
        "valid = torch.ones((1, 1, 62, 62), device=device)\n",
        "fake = torch.zeros((1, 1, 62, 62), device=device)\n",
        "\n",
        "# ------------------\n",
        "# Train Generator\n",
        "# ------------------\n",
        "optimizer_G.zero_grad()\n",
        "\n",
        "fake_face = G(collage)\n",
        "\n",
        "pred_fake = D(collage, fake_face)\n",
        "loss_GAN = criterion_GAN(pred_fake, valid)\n",
        "loss_L1 = criterion_L1(fake_face, target_face)\n",
        "\n",
        "loss_G = loss_GAN + 100 * loss_L1\n",
        "loss_G.backward()\n",
        "optimizer_G.step()\n",
        "\n",
        "# ------------------\n",
        "# Train Discriminator\n",
        "# ------------------\n",
        "optimizer_D.zero_grad()\n",
        "\n",
        "pred_real = D(collage, target_face)\n",
        "loss_real = criterion_GAN(pred_real, valid)\n",
        "\n",
        "pred_fake = D(collage, fake_face.detach())\n",
        "loss_fake = criterion_GAN(pred_fake, fake)\n",
        "\n",
        "loss_D = (loss_real + loss_fake) / 2\n",
        "loss_D.backward()\n",
        "optimizer_D.step()\n",
        "\n",
        "print(\"Loss G:\", loss_G.item(), \"Loss D:\", loss_D.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "et_JSKgzF9d4",
        "outputId": "417979b8-b4bf-499a-d8d9-fe6281792c56"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'collage' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4008977909.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Input Collage\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Generated Face\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'collage' is not defined"
          ]
        }
      ],
      "source": [
        "def show(tensor, title=\"\"):\n",
        "    img = tensor.squeeze().permute(1,2,0).cpu()\n",
        "    img = (img + 1) / 2\n",
        "    plt.imshow(img)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "show(collage, \"Input Collage\")\n",
        "show(fake_face, \"Generated Face\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlHSRKgIGBBA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}